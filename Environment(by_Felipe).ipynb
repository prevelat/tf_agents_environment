{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Environment(by Felipe).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prevelat/tf_agents_environment/blob/master/Environment(by_Felipe).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Ov2r-822EK",
        "colab_type": "text"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgyhzFwFnD5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "7a969658-ad16-43e9-aed3-bb110a80a67e"
      },
      "source": [
        "!pip install tf-agents-nightly"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-agents-nightly in /usr/local/lib/python3.6/dist-packages (0.2.0.dev20191216)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /tensorflow-2.1.0/python3.6 (from tf-agents-nightly) (1.17.4)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /tensorflow-2.1.0/python3.6 (from tf-agents-nightly) (0.8.1)\n",
            "Requirement already satisfied: gin-config==0.1.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents-nightly) (0.1.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /tensorflow-2.1.0/python3.6 (from tf-agents-nightly) (1.13.0)\n",
            "Requirement already satisfied: tfp-nightly in /usr/local/lib/python3.6/dist-packages (from tf-agents-nightly) (0.9.0.dev20191216)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.1)\n",
            "Requirement already satisfied: gast>=0.2 in /tensorflow-2.1.0/python3.6 (from tfp-nightly->tf-agents-nightly) (0.2.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tfp-nightly->tf-agents-nightly) (1.2.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQV9kBmMCJcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %%tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmm51s7k8M5m",
        "colab_type": "text"
      },
      "source": [
        "# HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl6vu-e78SPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iterations = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10000  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval =   50# @param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKdnPmJp26A5",
        "colab_type": "text"
      },
      "source": [
        "# ENVIRONMENT\n",
        "\n",
        "For this whole code this is the only part that I did it myself,\n",
        "as practice/learning all the other parts were based on the\n",
        "tensorflow agents tutorial by:\n",
        "https://github.com/tensorflow/agents/tree/master/tf_agents/colabs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0LC0OYiD6cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=0, maximum=5, name='observation')\n",
        "    self._state = 0\n",
        "    self._episode_ended = False\n",
        "\n",
        "    self._reward = 0\n",
        "\n",
        "    # Make sure it doesn't last forever\n",
        "    self._max_steps = 1000\n",
        "    self._n_step = 0\n",
        "\n",
        "    # Define state/action outcome: destination state and reward \n",
        "    self._action_outcome = {\n",
        "        0: {'up': {'reward': -1, 'dest_state': 0},\n",
        "            'right': {'reward': 0, 'dest_state': 1},\n",
        "            'down': {'reward': 0, 'dest_state': 3},\n",
        "            'left': {'reward': -1, 'dest_state': 0},\n",
        "        },\n",
        "        1: {'up': {'reward': -1, 'dest_state': 1},\n",
        "            'right': {'reward': 0, 'dest_state': 2},\n",
        "            'down': {'reward': 0, 'dest_state': 4},\n",
        "            'left': {'reward': 0, 'dest_state': 0},\n",
        "        },\n",
        "        2: {'up': {'reward': -1, 'dest_state': 2},\n",
        "            'right': {'reward': -1, 'dest_state': 2},\n",
        "            'down': {'reward': 0, 'dest_state': 5},\n",
        "            'left': {'reward': 0, 'dest_state': 1},\n",
        "        },\n",
        "        3: {'up': {'reward': 0, 'dest_state': 0},\n",
        "            'right': {'reward': 0, 'dest_state': 4},\n",
        "            'down': {'reward': -1, 'dest_state': 3},\n",
        "            'left': {'reward': -1, 'dest_state': 3},\n",
        "        },\n",
        "        4: {'up': {'reward': 0, 'dest_state': 1},\n",
        "            'right': {'reward': -5, 'dest_state': 5},\n",
        "            'down': {'reward': -1, 'dest_state': 4},\n",
        "            'left': {'reward': 0, 'dest_state': 3},\n",
        "        },\n",
        "        5: {'up': {'reward': 10, 'dest_state': 2},\n",
        "            'right': {'reward': -1, 'dest_state': 5},\n",
        "            'down': {'reward': -1, 'dest_state': 5},\n",
        "            'left': {'reward': 0, 'dest_state': 4},\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Define movement\n",
        "    self._movement = {\n",
        "        0: {'direction': 'up', 'action': False},\n",
        "        1: {'direction': 'right', 'action': False},\n",
        "        2: {'direction': 'down', 'action': False},\n",
        "        3: {'direction': 'left', 'action': False}\n",
        "    }\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._state = 0\n",
        "    self._n_step = 0\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
        "\n",
        "  def _move(self, direction):\n",
        "    for s in self._action_outcome:\n",
        "      if s == self._state:\n",
        "        self._reward = self._action_outcome[s][direction]['reward']\n",
        "        self._state = self._action_outcome[s][direction]['dest_state']\n",
        "\n",
        "  def _step(self, action):\n",
        "    # The last action ended the episode. Ignore the current action and start a new episode.\n",
        "    if self._state == 2 or self._n_step == self._max_steps:\n",
        "      return self._reset()\n",
        "\n",
        "    # Check if action is valid\n",
        "    if action < 0 or action > 3:\n",
        "      raise ValueError('`action` should be between 0 or 3')\n",
        "\n",
        "    # Get direction\n",
        "    self._movement[int(action)]['action'] = True\n",
        "\n",
        "    # Move\n",
        "    self._n_step += 1\n",
        "    for m in self._movement:\n",
        "      if self._movement[m]['action'] == True:\n",
        "        reward = self._move(self._movement[m]['direction'])\n",
        "        \n",
        "    # Reset direction\n",
        "    self._movement[int(action)]['action'] = False\n",
        "\n",
        "    if self._state == 2 or self._n_step == self._max_steps:\n",
        "      return ts.termination(np.array([self._state], dtype=np.int32), reward=self._reward)\n",
        "    else:\n",
        "      return ts.transition(np.array([self._state], dtype=np.int32), reward=self._reward, discount=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU2vuBQ3nLXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = myEnv()\n",
        "utils.validate_py_environment(env, episodes=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54o3yLL1rGKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_py_env = myEnv()\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "\n",
        "eval_py_env = myEnv()\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYEhEQKS2ukG",
        "colab_type": "text"
      },
      "source": [
        "# AGENT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h913oGeu2lDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fc_layer_params = (100,)\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N0xibfl2lMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHM4vDnv6BVu",
        "colab_type": "text"
      },
      "source": [
        "# POLICIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTi2Tsl_6Dug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7IExCu97BZh",
        "colab_type": "text"
      },
      "source": [
        "# METRICS AND EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGA9Zsqg7EoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNIKyZno3soC",
        "colab_type": "text"
      },
      "source": [
        "# REPLAY BUFFER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tse7U_ek2lgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_IXZz1a4xSF",
        "colab_type": "text"
      },
      "source": [
        "# DATA COLLECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo9OmFzB2lmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, steps=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8PrLU3U2lr-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "003a6c42-b3f4-47e9-f688-7feff3e5078b"
      },
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 1), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.int32, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no-2Pd6dxqlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f60a1076-c454-4322-9926-d30c7b922e26"
      },
      "source": [
        "iterator = iter(dataset)\n",
        "\n",
        "print(iterator)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f701c1cf128>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAAg8Hyf6w_J",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUCAC0AC2lxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bb0a13a-014d-4db4-8819-c63d82c7f648"
      },
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  for _ in range(collect_steps_per_iteration):\n",
        "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step = 50: Average Return = 8.0\n",
            "step = 100: Average Return = 8.0\n",
            "step = 150: Average Return = -1000.0\n",
            "step = 200: loss = 1.7589154243469238\n",
            "step = 200: Average Return = 8.0\n",
            "step = 250: Average Return = -1000.0\n",
            "step = 300: Average Return = 8.0\n",
            "step = 350: Average Return = 8.0\n",
            "step = 400: loss = 1.2600901126861572\n",
            "step = 400: Average Return = -1000.0\n",
            "step = 450: Average Return = 8.0\n",
            "step = 500: Average Return = 8.0\n",
            "step = 550: Average Return = -1000.0\n",
            "step = 600: loss = 0.3073885142803192\n",
            "step = 600: Average Return = 8.0\n",
            "step = 650: Average Return = 8.0\n",
            "step = 700: Average Return = 8.0\n",
            "step = 750: Average Return = 8.0\n",
            "step = 800: loss = 0.015352899208664894\n",
            "step = 800: Average Return = 8.0\n",
            "step = 850: Average Return = 8.0\n",
            "step = 900: Average Return = 8.0\n",
            "step = 950: Average Return = 8.0\n",
            "step = 1000: loss = 0.008574571460485458\n",
            "step = 1000: Average Return = 8.0\n",
            "step = 1050: Average Return = 8.0\n",
            "step = 1100: Average Return = 8.0\n",
            "step = 1150: Average Return = 8.0\n",
            "step = 1200: loss = 0.021120645105838776\n",
            "step = 1200: Average Return = 8.0\n",
            "step = 1250: Average Return = 8.0\n",
            "step = 1300: Average Return = 8.0\n",
            "step = 1350: Average Return = 8.0\n",
            "step = 1400: loss = 0.1758345663547516\n",
            "step = 1400: Average Return = 8.0\n",
            "step = 1450: Average Return = 8.0\n",
            "step = 1500: Average Return = 8.0\n",
            "step = 1550: Average Return = 8.0\n",
            "step = 1600: loss = 0.47251996397972107\n",
            "step = 1600: Average Return = 8.0\n",
            "step = 1650: Average Return = 8.0\n",
            "step = 1700: Average Return = 8.0\n",
            "step = 1750: Average Return = 8.0\n",
            "step = 1800: loss = 3.845562241622247e-05\n",
            "step = 1800: Average Return = 8.0\n",
            "step = 1850: Average Return = 8.0\n",
            "step = 1900: Average Return = 8.0\n",
            "step = 1950: Average Return = 8.0\n",
            "step = 2000: loss = 0.08990350365638733\n",
            "step = 2000: Average Return = 8.0\n",
            "step = 2050: Average Return = 8.0\n",
            "step = 2100: Average Return = 8.0\n",
            "step = 2150: Average Return = 8.0\n",
            "step = 2200: loss = 0.234712153673172\n",
            "step = 2200: Average Return = 8.0\n",
            "step = 2250: Average Return = 8.0\n",
            "step = 2300: Average Return = 8.0\n",
            "step = 2350: Average Return = 8.0\n",
            "step = 2400: loss = 0.05962826684117317\n",
            "step = 2400: Average Return = 8.0\n",
            "step = 2450: Average Return = 8.0\n",
            "step = 2500: Average Return = 8.0\n",
            "step = 2550: Average Return = 8.0\n",
            "step = 2600: loss = 0.041052550077438354\n",
            "step = 2600: Average Return = 8.0\n",
            "step = 2650: Average Return = 8.0\n",
            "step = 2700: Average Return = 8.0\n",
            "step = 2750: Average Return = 8.0\n",
            "step = 2800: loss = 0.0010669664479792118\n",
            "step = 2800: Average Return = 8.0\n",
            "step = 2850: Average Return = 8.0\n",
            "step = 2900: Average Return = 8.0\n",
            "step = 2950: Average Return = 8.0\n",
            "step = 3000: loss = 0.02932657115161419\n",
            "step = 3000: Average Return = 8.0\n",
            "step = 3050: Average Return = 8.0\n",
            "step = 3100: Average Return = 8.0\n",
            "step = 3150: Average Return = 8.0\n",
            "step = 3200: loss = 0.011636143550276756\n",
            "step = 3200: Average Return = 8.0\n",
            "step = 3250: Average Return = 8.0\n",
            "step = 3300: Average Return = 8.0\n",
            "step = 3350: Average Return = 8.0\n",
            "step = 3400: loss = 0.006649056449532509\n",
            "step = 3400: Average Return = 8.0\n",
            "step = 3450: Average Return = 8.0\n",
            "step = 3500: Average Return = 8.0\n",
            "step = 3550: Average Return = 8.0\n",
            "step = 3600: loss = 0.05019432306289673\n",
            "step = 3600: Average Return = 8.0\n",
            "step = 3650: Average Return = 8.0\n",
            "step = 3700: Average Return = 8.0\n",
            "step = 3750: Average Return = 8.0\n",
            "step = 3800: loss = 0.023450437933206558\n",
            "step = 3800: Average Return = 8.0\n",
            "step = 3850: Average Return = 8.0\n",
            "step = 3900: Average Return = 8.0\n",
            "step = 3950: Average Return = 8.0\n",
            "step = 4000: loss = 0.03435608372092247\n",
            "step = 4000: Average Return = 8.0\n",
            "step = 4050: Average Return = 8.0\n",
            "step = 4100: Average Return = 8.0\n",
            "step = 4150: Average Return = 8.0\n",
            "step = 4200: loss = 0.0007483088411390781\n",
            "step = 4200: Average Return = 8.0\n",
            "step = 4250: Average Return = 8.0\n",
            "step = 4300: Average Return = 8.0\n",
            "step = 4350: Average Return = 8.0\n",
            "step = 4400: loss = 0.009753193706274033\n",
            "step = 4400: Average Return = 8.0\n",
            "step = 4450: Average Return = 8.0\n",
            "step = 4500: Average Return = 8.0\n",
            "step = 4550: Average Return = 8.0\n",
            "step = 4600: loss = 0.0023969640024006367\n",
            "step = 4600: Average Return = 8.0\n",
            "step = 4650: Average Return = 8.0\n",
            "step = 4700: Average Return = 8.0\n",
            "step = 4750: Average Return = 8.0\n",
            "step = 4800: loss = 2.2530653950525448e-05\n",
            "step = 4800: Average Return = 8.0\n",
            "step = 4850: Average Return = 8.0\n",
            "step = 4900: Average Return = 8.0\n",
            "step = 4950: Average Return = 8.0\n",
            "step = 5000: loss = 0.0013386935461312532\n",
            "step = 5000: Average Return = 8.0\n",
            "step = 5050: Average Return = 8.0\n",
            "step = 5100: Average Return = 8.0\n",
            "step = 5150: Average Return = 8.0\n",
            "step = 5200: loss = 1.6293026419589296e-05\n",
            "step = 5200: Average Return = 8.0\n",
            "step = 5250: Average Return = 8.0\n",
            "step = 5300: Average Return = 8.0\n",
            "step = 5350: Average Return = 8.0\n",
            "step = 5400: loss = 0.007476138416677713\n",
            "step = 5400: Average Return = 8.0\n",
            "step = 5450: Average Return = 8.0\n",
            "step = 5500: Average Return = 8.0\n",
            "step = 5550: Average Return = 8.0\n",
            "step = 5600: loss = 0.0013711501378566027\n",
            "step = 5600: Average Return = 8.0\n",
            "step = 5650: Average Return = 8.0\n",
            "step = 5700: Average Return = 8.0\n",
            "step = 5750: Average Return = 8.0\n",
            "step = 5800: loss = 8.370599061890971e-06\n",
            "step = 5800: Average Return = 8.0\n",
            "step = 5850: Average Return = 8.0\n",
            "step = 5900: Average Return = 8.0\n",
            "step = 5950: Average Return = 8.0\n",
            "step = 6000: loss = 8.323093425133266e-06\n",
            "step = 6000: Average Return = 8.0\n",
            "step = 6050: Average Return = 8.0\n",
            "step = 6100: Average Return = 8.0\n",
            "step = 6150: Average Return = 8.0\n",
            "step = 6200: loss = 9.728089935379103e-05\n",
            "step = 6200: Average Return = 8.0\n",
            "step = 6250: Average Return = 8.0\n",
            "step = 6300: Average Return = 8.0\n",
            "step = 6350: Average Return = 8.0\n",
            "step = 6400: loss = 2.9909546128692455e-07\n",
            "step = 6400: Average Return = 8.0\n",
            "step = 6450: Average Return = 8.0\n",
            "step = 6500: Average Return = 8.0\n",
            "step = 6550: Average Return = 8.0\n",
            "step = 6600: loss = 1.4526232916978188e-05\n",
            "step = 6600: Average Return = 8.0\n",
            "step = 6650: Average Return = 8.0\n",
            "step = 6700: Average Return = 8.0\n",
            "step = 6750: Average Return = 8.0\n",
            "step = 6800: loss = 3.755224679480307e-05\n",
            "step = 6800: Average Return = 8.0\n",
            "step = 6850: Average Return = 8.0\n",
            "step = 6900: Average Return = 8.0\n",
            "step = 6950: Average Return = 8.0\n",
            "step = 7000: loss = 3.2787834243208636e-06\n",
            "step = 7000: Average Return = 8.0\n",
            "step = 7050: Average Return = 8.0\n",
            "step = 7100: Average Return = 8.0\n",
            "step = 7150: Average Return = 8.0\n",
            "step = 7200: loss = 4.6944900532253087e-05\n",
            "step = 7200: Average Return = 8.0\n",
            "step = 7250: Average Return = 8.0\n",
            "step = 7300: Average Return = 8.0\n",
            "step = 7350: Average Return = 8.0\n",
            "step = 7400: loss = 0.00011853664182126522\n",
            "step = 7400: Average Return = 8.0\n",
            "step = 7450: Average Return = 8.0\n",
            "step = 7500: Average Return = 8.0\n",
            "step = 7550: Average Return = 8.0\n",
            "step = 7600: loss = 1.6430792726396248e-08\n",
            "step = 7600: Average Return = 8.0\n",
            "step = 7650: Average Return = 8.0\n",
            "step = 7700: Average Return = 8.0\n",
            "step = 7750: Average Return = 8.0\n",
            "step = 7800: loss = 1.770423841662705e-05\n",
            "step = 7800: Average Return = 8.0\n",
            "step = 7850: Average Return = 8.0\n",
            "step = 7900: Average Return = 8.0\n",
            "step = 7950: Average Return = 8.0\n",
            "step = 8000: loss = 8.583208909840323e-06\n",
            "step = 8000: Average Return = 8.0\n",
            "step = 8050: Average Return = 8.0\n",
            "step = 8100: Average Return = 8.0\n",
            "step = 8150: Average Return = 8.0\n",
            "step = 8200: loss = 0.0004811687977053225\n",
            "step = 8200: Average Return = 8.0\n",
            "step = 8250: Average Return = 8.0\n",
            "step = 8300: Average Return = 8.0\n",
            "step = 8350: Average Return = 8.0\n",
            "step = 8400: loss = 4.144748970702494e-07\n",
            "step = 8400: Average Return = 8.0\n",
            "step = 8450: Average Return = 8.0\n",
            "step = 8500: Average Return = 8.0\n",
            "step = 8550: Average Return = 8.0\n",
            "step = 8600: loss = 1.6477646568091586e-05\n",
            "step = 8600: Average Return = 8.0\n",
            "step = 8650: Average Return = 8.0\n",
            "step = 8700: Average Return = 8.0\n",
            "step = 8750: Average Return = 8.0\n",
            "step = 8800: loss = 0.0002320824278285727\n",
            "step = 8800: Average Return = 8.0\n",
            "step = 8850: Average Return = 8.0\n",
            "step = 8900: Average Return = 8.0\n",
            "step = 8950: Average Return = 8.0\n",
            "step = 9000: loss = 0.00018613968859426677\n",
            "step = 9000: Average Return = 8.0\n",
            "step = 9050: Average Return = 8.0\n",
            "step = 9100: Average Return = 8.0\n",
            "step = 9150: Average Return = 8.0\n",
            "step = 9200: loss = 1.0786930033646058e-06\n",
            "step = 9200: Average Return = 8.0\n",
            "step = 9250: Average Return = 8.0\n",
            "step = 9300: Average Return = 8.0\n",
            "step = 9350: Average Return = 8.0\n",
            "step = 9400: loss = 0.00026110641192644835\n",
            "step = 9400: Average Return = 8.0\n",
            "step = 9450: Average Return = 8.0\n",
            "step = 9500: Average Return = 8.0\n",
            "step = 9550: Average Return = 8.0\n",
            "step = 9600: loss = 0.00030212372075766325\n",
            "step = 9600: Average Return = 8.0\n",
            "step = 9650: Average Return = 8.0\n",
            "step = 9700: Average Return = 8.0\n",
            "step = 9750: Average Return = 8.0\n",
            "step = 9800: loss = 0.0002090886700898409\n",
            "step = 9800: Average Return = 8.0\n",
            "step = 9850: Average Return = 8.0\n",
            "step = 9900: Average Return = 8.0\n",
            "step = 9950: Average Return = 8.0\n",
            "step = 10000: loss = 7.419354005833156e-06\n",
            "step = 10000: Average Return = 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xHZ99g07Zaw",
        "colab_type": "text"
      },
      "source": [
        "# VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH2r_6lC2l3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "83de5c8a-9c06-4e7b-b2a7-417064f868d3"
      },
      "source": [
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1050.4, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEGCAYAAABCa2PoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAcXMzEbCAcXMzEbi/wNLfMwChOTsGQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCLl209d2l9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XLTBIFL2mDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODCxcCvz2mJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCjLOHiW2mPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2yroH-v2mVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "  # def verbose(self):\n",
        "    \n",
        "  #   if self._state == 2:\n",
        "  #     print(\"----- END OF EPISODE -----\")\n",
        "  #   else:\n",
        "  #     print(\"Current State:\", self._state, \", Last Reward:\", self._reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbkJo4dINdEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hello = {\n",
        "#     0: {\n",
        "#         'direction': 'up',\n",
        "#         'action': False\n",
        "#     },\n",
        "#     1: {\n",
        "#         'direction': 'right',\n",
        "#         'action': False\n",
        "#     },\n",
        "#     2: {\n",
        "#         'direction': 'down',\n",
        "#         'action': False\n",
        "#     },\n",
        "#     3: {\n",
        "#         'direction': 'left',\n",
        "#         'action': False\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# movement = {\n",
        "#     0: {'direction': 'up', 'action': False},\n",
        "#     1: {'direction': 'right', 'action': False},\n",
        "#     2: {'direction': 'down', 'action': False},\n",
        "#     3: {'direction': 'left', 'action': False}\n",
        "# }\n",
        "\n",
        "# action_outcome = {\n",
        "#     0: {'up': {'reward': -1, 'dest_state': 0},\n",
        "#         'right': {'reward': 0, 'dest_state': 1},\n",
        "#         'down': {'reward': 0, 'dest_state': 3},\n",
        "#         'left': {'reward': -1, 'dest_state': 0},\n",
        "#     },\n",
        "#     1: {'up': {'reward': -1, 'dest_state': 1},\n",
        "#         'right': {'reward': 0, 'dest_state': 2},\n",
        "#         'down': {'reward': 0, 'dest_state': 4},\n",
        "#         'left': {'reward': 0, 'dest_state': 0},\n",
        "#     },\n",
        "#     2: {'up': {'reward': -1, 'dest_state': 2},\n",
        "#         'right': {'reward': -1, 'dest_state': 2},\n",
        "#         'down': {'reward': 0, 'dest_state': 5},\n",
        "#         'left': {'reward': 0, 'dest_state': 1},\n",
        "#     },\n",
        "#     3: {'up': {'reward': 0, 'dest_state': 0},\n",
        "#         'right': {'reward': 0, 'dest_state': 4},\n",
        "#         'down': {'reward': -1, 'dest_state': 3},\n",
        "#         'left': {'reward': -1, 'dest_state': 3},\n",
        "#     },\n",
        "#     4: {'up': {'reward': 0, 'dest_state': 1},\n",
        "#         'right': {'reward': -5, 'dest_state': 5},\n",
        "#         'down': {'reward': -1, 'dest_state': 4},\n",
        "#         'left': {'reward': 0, 'dest_state': 3},\n",
        "#     },\n",
        "#     5: {'up': {'reward': 10, 'dest_state': 2},\n",
        "#         'right': {'reward': -1, 'dest_state': 5},\n",
        "#         'down': {'reward': -1, 'dest_state': 5},\n",
        "#         'left': {'reward': 0, 'dest_state': 4},\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# state = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNxJZkyvP314",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# action = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucMR4DOUN4CJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hello[int(action)]['action'] = True\n",
        "\n",
        "# hello"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUpRMArXmI-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def move(direction):\n",
        "#   for s in action_outcome:\n",
        "#     if s == state:\n",
        "#       print(action_outcome[s][direction]['reward'], action_outcome[s][direction]['dest_state'])\n",
        "\n",
        "#   print(direction)\n",
        "  # for d in self._action_outcome:\n",
        "  #   if d == direction:\n",
        "  #     self._reward = d['reward']\n",
        "  #     self._state = d['dest_state']\n",
        "\n",
        "# for m in hello:\n",
        "#   if hello[m]['action'] == True:\n",
        "#     move(hello[m]['direction'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}